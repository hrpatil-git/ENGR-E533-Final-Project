{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2087210d-e457-4d62-943e-9b4809d780d1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b747256-36fe-4050-92ac-c668e6bb2111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot  as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Input, Softmax, RNN, Dense, Embedding, LSTM, Layer, Dropout "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c3b97bf-5525-4234-8567-003f377d4970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4948b880-4f3b-42d6-b2f4-481e86457dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ecb815-e09c-4498-80b7-ea4537d2269a",
   "metadata": {},
   "source": [
    "# Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0ad5e54-a80c-475e-a2fb-6475beceeab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model =  tf.keras.applications.DenseNet121(weights='CheXNet_weights.h5', classes = 14, input_shape=(256,256,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50b3493a-abf6-40bf-b316-250d01414059",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_chest_x_net = Model(image_model.input, image_model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afff5582-95fa-4059-9782-0816f3a551d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = np.load('df_new.npy',allow_pickle=True)\n",
    "\n",
    "train_split = merged_df[:3000] \n",
    "test_split = merged_df[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59674283-7cc3-4b93-80c9-51eaf0de3371",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_split.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "623e0828-04f8-4648-b130-58cebce7584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = pd.DataFrame(train_split, columns = ['front_view', 'lateral_view', 'findings','dec_ip','dec_op','extracted_feature'])\n",
    "test_split = pd.DataFrame(test_split, columns = ['front_view', 'lateral_view', 'findings','dec_ip','dec_op','extracted_feature'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1721f2e5-0495-4925-97d3-e0ef8f7faf70",
   "metadata": {},
   "source": [
    "Tokennizing the finding columns to frequency space to easly compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0976a93e-930c-4205-a3f7-39d7eda105fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size - 1399\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Tokenizer with specified filters\n",
    "tokn_obj = Tokenizer(filters='!\"#$%&()*+,-/:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "\n",
    "# Fit Tokenizer on the 'findings' column of the 'train' DataFrame\n",
    "tokn_obj.fit_on_texts(train_split['findings'])\n",
    "\n",
    "# Add padding token to the word index\n",
    "padd_obje = '<pad>'\n",
    "tokn_obj.word_index[padd_obje] = 0\n",
    "tokn_obj.index_word[0] = padd_obje\n",
    "\n",
    "# Calculate the vocabulary size\n",
    "len_all_words = len(tokn_obj.word_index) + 1\n",
    "\n",
    "# Print the vocabulary size\n",
    "print('Vocab size -', len_all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f44e50df-0d95-480a-8ca5-534b35371619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5  16   4  20  74  13   3   2  18  24   9  14  10 104  29  25   3  19\n",
      "   7  21   4  34   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# Sequence in train and validation\n",
    "decoder_tokn_inp = tokn_obj.texts_to_sequences(train_split.dec_ip)\n",
    "decoder_tokn_opt = tokn_obj.texts_to_sequences(train_split.dec_op)\n",
    "\n",
    "test_decoder_tokn_inp = tokn_obj.texts_to_sequences(test_split.dec_ip)\n",
    "test_decoder_tokn_opt = tokn_obj.texts_to_sequences(test_split.dec_op)\n",
    "\n",
    "# Padding in the train and validation\n",
    "max_len = 100\n",
    "final_decoder_input = keras.preprocessing.sequence.pad_sequences(decoder_tokn_inp, maxlen=max_len, padding='post')\n",
    "final_decoder_output = keras.preprocessing.sequence.pad_sequences(decoder_tokn_opt, maxlen=max_len, padding='post') \n",
    "\n",
    "final_test_decoder_input = keras.preprocessing.sequence.pad_sequences(test_decoder_tokn_inp, maxlen=max_len, padding='post')\n",
    "final_test_decoder_output = keras.preprocessing.sequence.pad_sequences(test_decoder_tokn_opt, maxlen=max_len, padding='post') \n",
    "\n",
    "print(final_decoder_input[100])\n",
    "\n",
    "# Create two inverse dictionalries of token_index in list and index in token object for easy search queries\n",
    "obj_place = {obj: place for obj, place in tokn_obj.word_index.items()}\n",
    "place_obj = {place: obj for obj, place in obj_place.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f65fa77c-227d-4992-b4c9-77fb09648c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_split.extracted_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341d5be1-b7e1-45ce-b4e9-a47c106facb6",
   "metadata": {},
   "source": [
    "# Text encoder decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70054aec-da8b-4c08-9db9-46b9cce611c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1399"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "481f64b7-ec0d-4042-87b8-3e7a38611f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_lstm = 256\n",
    "batch = 64\n",
    "Buffer_words = 500\n",
    "latent_dim = 300\n",
    "attentionblock = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "72c18185-1562-4b60-acb0-e6793476c391",
   "metadata": {},
   "outputs": [],
   "source": [
    "class primary_E_block(Model):\n",
    "    def __init__(self,block_lstm):\n",
    "        super().__init__()\n",
    "        self.layer1  = Dense(block_lstm, kernel_initializer=\"glorot_uniform\",)\n",
    "        \n",
    "    def call(self,x):\n",
    "      encoder_output = self.layer1(x)\n",
    "      return encoder_output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e2190ff-0d6b-4114-9a8b-6eca7e2abd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Primary_D_block(Model):\n",
    "    def __init__(self, len_all_words, latent_dim, block_lstm, attentionblock):\n",
    "          \n",
    "          super().__init__()\n",
    "\n",
    "          self.block_lstm = block_lstm\n",
    "          self.len_all_words = len_all_words\n",
    "          self.latent_dim = latent_dim\n",
    "          self.attentionblock = attentionblock\n",
    "          \n",
    "          self.jump_decode = jump_decode(self.len_all_words, self.latent_dim, self.block_lstm, self.attentionblock)\n",
    "\n",
    "    # @tf.function\n",
    "    def call(self, x, training=None):\n",
    "        \n",
    "        io_decode, e_block_op, decoder_hidden = x\n",
    "        batch_op = tf.TensorArray(tf.float32,size = io_decode.shape[1])\n",
    "        batch_list = []\n",
    "        for window in tf.range(io_decode.shape[1]):\n",
    "            y=io_decode[:,window:window+1]\n",
    "            output,middle_st,weights,sentance_vector = self.jump_decode((y,e_block_op,decoder_hidden))\n",
    "            # output = tf.round(output)\n",
    "            # output = tf.cast(output, tf.int32)\n",
    "            batch_op = batch_op.write(window,output)\n",
    "            # batch_list.append((window,output))\n",
    "            # batch_list.append(output)\n",
    "        \n",
    "        # batch_op = tf.unstack(batch_op)\n",
    "        # batch_op = tf.stack(batch_op)\n",
    "        # tensors_temp = tf.unstack(batch_op)\n",
    "        # print(batch_op)\n",
    "        \n",
    "        transposed_tensors = [batch_op.read(i) for i in range(io_decode.shape[1])]\n",
    "        batch_op = tf.stack(transposed_tensors)\n",
    "        batch_op = tf.transpose(batch_op, perm=[1, 0, 2])\n",
    "        \n",
    "        return batch_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8427f7f-1c3d-45b6-9ff0-c2b58890d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_block_class(Model):\n",
    "    def __init__(self,attentionblock):\n",
    "        super().__init__()\n",
    "        self.attentionblock = attentionblock\n",
    "        # self.multi_headed_attention = tf.keras.layers.MultiHeadAttention(2,key_dim= attentionblock//2)\n",
    "        self.layyer1=  Dense(self.attentionblock, kernel_initializer=\"glorot_uniform\", name='atn1')\n",
    "        self.layyer2 =  Dense(self.attentionblock, kernel_initializer=\"glorot_uniform\", name='atn2')\n",
    "        self.layyer3 = Dense(1, kernel_initializer=\"glorot_uniform\", name = 'atn3')\n",
    "    \n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        decoder_hidden, e_block_op = x\n",
    "        decoder_hidden = tf.expand_dims(decoder_hidden,axis = 1)\n",
    "\n",
    "        # e_block_op = tf.reshape(e_block_op, (tf.shape(e_block_op)[0], -1, 2, self.attentionblock // 2))\n",
    "        \n",
    "        sam_OP = self.layyer3(tf.nn.tanh(self.layyer1(decoder_hidden) + self.layyer2(e_block_op)))\n",
    "        \n",
    "        A_weights    = tf.nn.softmax(sam_OP, axis=1)\n",
    "        word_vector = A_weights * e_block_op\n",
    "        word_vector = tf.reduce_sum(word_vector, axis=1)\n",
    "        \n",
    "        return word_vector, A_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4c7bf8a-301a-429d-8113-09252fc34d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class jump_decode(Model):\n",
    "  def __init__(self, len_all_words, latent_dim, block_lstm, attentionblock):\n",
    "      super().__init__()\n",
    "      \n",
    "      self.len_all_words = len_all_words\n",
    "      self.latent_dim = latent_dim\n",
    "      self.block_lstm = block_lstm\n",
    "      self.attentionblock = attentionblock\n",
    "      \n",
    "      self.layerd = Dense(self.len_all_words, kernel_initializer=\"glorot_uniform\")\n",
    "      self.attentio_block = Attention_block_class( self.attentionblock)\n",
    "      self.dec_emb = Embedding(self.len_all_words, self.latent_dim, trainable = True , name = 'embedding')           \n",
    "      self.dec_gru = GRU(self.block_lstm, return_state=True, return_sequences=True, name=\"DLSTM\") \n",
    "      \n",
    "      \n",
    "      self.dp1 = Dropout(0.2,name = 'd1')\n",
    "      self.dp2 = Dropout(0.2,name = 'd2')\n",
    "      self.dp3 = Dropout(0.2,name = 'd3')\n",
    "  \n",
    "  @tf.function\n",
    "  def call(self,x,training=None):\n",
    "    \n",
    "    io_decode, e_block_op, dec_hidden = x\n",
    "\n",
    "    embedded_output = self.dec_emb(io_decode)\n",
    "    embedded_output = self.dp1(embedded_output)\n",
    "    \n",
    "   \n",
    "    word_vector, layer_weights = self.attentio_block([dec_hidden,e_block_op])\n",
    "\n",
    "    comp_dec_ip = tf.concat([tf.expand_dims(word_vector, 1),embedded_output], -1)\n",
    "    comp_dec_ip = self.dp2(comp_dec_ip)\n",
    "\n",
    "    gru_op, gru_hidden = self.dec_gru(comp_dec_ip, initial_state=dec_hidden)\n",
    "    \n",
    "    gru_op = tf.reshape(gru_op, (-1, gru_op.shape[2]))\n",
    "    gru_op = self.dp3(gru_op)\n",
    "\n",
    "    output = self.layerd(gru_op)\n",
    "\n",
    "    return output,gru_hidden,layer_weights,word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "403d0f63-b751-4d51-8c91-456968384181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combine_model(Model):\n",
    "\n",
    "  def __init__(self, len_all_words, latent_dim, block_lstm, attentionblock, batch):\n",
    "        super().__init__()\n",
    "\n",
    "        self.len_all_words = len_all_words\n",
    "        self.batch = batch\n",
    "        self.block_lstm = block_lstm\n",
    "        self.latent_dim = latent_dim\n",
    "        self.attentionblock = attentionblock\n",
    "\n",
    "        self.encoder = primary_E_block(self.block_lstm)\n",
    "        self.decoder = Primary_D_block(len_all_words, latent_dim, block_lstm, attentionblock)\n",
    "        self.layerd   = Dense(self.len_all_words, kernel_initializer=\"glorot_uniform\", name = 'last_layer_dense')\n",
    "\n",
    "\n",
    "  \n",
    "  def call(self,X):\n",
    "    \n",
    "    inbase, outbase = X[0], X[1]\n",
    "\n",
    "    encoder_middle = tf.zeros((self.batch, self.block_lstm))\n",
    "      \n",
    "    enc_output = self.encoder(inbase)\n",
    "    print(enc_output.shape)\n",
    "    comb_x = [outbase,enc_output,encoder_middle]\n",
    "    output = self.decoder(comb_x)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0a4bd87-b479-4154-92dc-094d1ef6f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def Custom_loss(orig,pred):\n",
    "  print(orig.shape,pred.shape)  \n",
    "  flter = tf.math.logical_not(tf.math.equal(orig, 0))\n",
    "  hloss_ans = hloss(orig, pred)\n",
    "  flter = tf.cast(flter, dtype=hloss_ans.dtype)\n",
    "  hloss_ans *= flter\n",
    "\n",
    "  return tf.reduce_mean(hloss_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab9074e3-4833-4905-b9e9-2ee9a24bcb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshaping the Image tensors for training\n",
    "train_image_features = np.vstack(train_split.extracted_feature)\n",
    "test_image_features = np.vstack(test_split.extracted_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06d83cfe-97ff-43ea-89f7-c1c0a9517958",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = ((train_image_features, final_decoder_input), final_decoder_output)\n",
    "test_df = ((test_image_features,final_test_decoder_input),final_test_decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f798fdfe-562a-46c7-baea-7c2fe01e0927",
   "metadata": {},
   "source": [
    "# problem area in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74212268-0114-44c0-9107-e2beccadde8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = tf.data.Dataset.from_tensor_slices(((train_image_features, final_decoder_input), final_decoder_output))\n",
    "train_df = train_df.shuffle(Buffer_words).batch(batch).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "test_df = tf.data.Dataset.from_tensor_slices(((test_image_features,final_test_decoder_input),final_test_decoder_output))\n",
    "test_df = test_df.shuffle(Buffer_words).batch(batch).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16f8a862-10b2-4961-8d60-ba38ca962a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(train_df[0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e36274e-7c6d-4b58-a9ec-aaa5838edaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attention_model = Combine_model(attentionblock=attentionblock,len_all_words =len_all_words, latent_dim=latent_dim,  batch=batch, block_lstm=block_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2f51680a-cc54-4e80-b636-c184205322d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Attention_model.compile(optimizer=tf.keras.optimizers.Adam(0.001),loss=Custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ccb9daab-9316-46b1-88f3-5cfb31c613dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_measure = tf.keras.callbacks.EarlyStopping(monitor='val_loss',  patience = 5, baseline=None, verbose = 1, restore_best_weights=True)\n",
    "adaptive_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', min_lr=0.000001, factor=0.1, patience=5, mode = 'min',verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e299d25-4839-44a1-846e-e1a8247c8a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "(None, 256)\n",
      "(None, 100) (64, 100, 1399)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/combine_model_1/primary_d_block_1/while/gradients/combine_model_1/primary_d_block_1/while/jump_decode_1/StatefulPartitionedCall_grad/PartitionedCall:3\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/combine_model_1/primary_d_block_1/while/gradients/combine_model_1/primary_d_block_1/while/jump_decode_1/StatefulPartitionedCall_grad/PartitionedCall:2\", shape=(None, 300), dtype=float32), dense_shape=Tensor(\"gradient_tape/combine_model_1/primary_d_block_1/while/gradients/combine_model_1/primary_d_block_1/while/jump_decode_1/StatefulPartitionedCall_grad/PartitionedCall:4\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 256)\n",
      "(None, 100) (64, 100, 1399)\n",
      "46/47 [============================>.] - ETA: 3s - loss: 0.7925"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'Custom_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert' defined at (most recent call last):\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 1046, in launch_instance\n      app.start()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Temp\\ipykernel_15988\\101232946.py\", line 1, in <module>\n      md_history = Attention_model.fit(train_df, validation_data=0.2, batch_size=64,epochs=100, callbacks=[early_measure,adaptive_lr], shuffle=True)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Temp\\ipykernel_15988\\2361301951.py\", line 6, in Custom_loss\n      hloss_ans = hloss(orig, pred)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 2084, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5630, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'Custom_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert'\nassertion failed: [Condition x == y did not hold element-wise:] [x (Custom_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/Shape_1:0) = ] [56 100] [y (Custom_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/strided_slice:0) = ] [64 100]\n\t [[{{node Custom_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert}}]] [Op:__inference_train_function_21075]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m md_history \u001b[38;5;241m=\u001b[39m \u001b[43mAttention_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_measure\u001b[49m\u001b[43m,\u001b[49m\u001b[43madaptive_lr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'Custom_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert' defined at (most recent call last):\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\traitlets\\config\\application.py\", line 1046, in launch_instance\n      app.start()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Temp\\ipykernel_15988\\101232946.py\", line 1, in <module>\n      md_history = Attention_model.fit(train_df, validation_data=0.2, batch_size=64,epochs=100, callbacks=[early_measure,adaptive_lr], shuffle=True)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 994, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1052, in compute_loss\n      return self.compiled_loss(\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Temp\\ipykernel_15988\\2361301951.py\", line 6, in Custom_loss\n      hloss_ans = hloss(orig, pred)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 272, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 2084, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"C:\\Users\\k2lea\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\backend.py\", line 5630, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'Custom_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert'\nassertion failed: [Condition x == y did not hold element-wise:] [x (Custom_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/Shape_1:0) = ] [56 100] [y (Custom_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/strided_slice:0) = ] [64 100]\n\t [[{{node Custom_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert}}]] [Op:__inference_train_function_21075]"
     ]
    }
   ],
   "source": [
    "md_history = Attention_model.fit(train_df, validation_data=0.2, batch_size=64,epochs=100, callbacks=[early_measure,adaptive_lr], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bb0511-8e48-486e-ae95-69665b3b7840",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
